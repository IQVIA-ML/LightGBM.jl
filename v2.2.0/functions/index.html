<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Functions · LightGBM.jl</title><meta name="title" content="Functions · LightGBM.jl"/><meta property="og:title" content="Functions · LightGBM.jl"/><meta property="twitter:title" content="Functions · LightGBM.jl"/><meta name="description" content="Documentation for LightGBM.jl."/><meta property="og:description" content="Documentation for LightGBM.jl."/><meta property="twitter:description" content="Documentation for LightGBM.jl."/><meta property="og:url" content="https://IQVIA-ML.github.io/LightGBM.jl/functions/"/><meta property="twitter:url" content="https://IQVIA-ML.github.io/LightGBM.jl/functions/"/><link rel="canonical" href="https://IQVIA-ML.github.io/LightGBM.jl/functions/"/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">LightGBM.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li class="is-active"><a class="tocitem" href>Functions</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Functions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/master/docs/src/functions.md#L" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Functions"><a class="docs-heading-anchor" href="#Functions">Functions</a><a id="Functions-1"></a><a class="docs-heading-anchor-permalink" href="#Functions" title="Permalink"></a></h1><ul><li><a href="#LightGBM.LGBMClassification-Tuple{}"><code>LightGBM.LGBMClassification</code></a></li><li><a href="#LightGBM.LGBMRanking-Tuple{}"><code>LightGBM.LGBMRanking</code></a></li><li><a href="#LightGBM.LGBMRegression-Tuple{}"><code>LightGBM.LGBMRegression</code></a></li><li><a href="#LightGBM.MLJInterface.LGBMClassifier"><code>LightGBM.MLJInterface.LGBMClassifier</code></a></li><li><a href="#LightGBM.MLJInterface.LGBMRegressor"><code>LightGBM.MLJInterface.LGBMRegressor</code></a></li><li><a href="#LightGBM.LGBM_BoosterUpdateOneIterCustom-Tuple{LightGBM.Booster, Vector{&lt;:AbstractFloat}, Vector{&lt;:AbstractFloat}}"><code>LightGBM.LGBM_BoosterUpdateOneIterCustom</code></a></li><li><a href="#LightGBM.cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any}} where {TX&lt;:Real, Ty&lt;:Real}"><code>LightGBM.cv</code></a></li><li><a href="#LightGBM.fit!-Union{Tuple{Ti}, Tuple{Tw}, Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}, Vector{Ty}, Vararg{Tuple{Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}, Vector{Ty}}}}} where {TX&lt;:Real, Ty&lt;:Real, Tw&lt;:Real, Ti&lt;:Real}"><code>LightGBM.fit!</code></a></li><li><a href="#LightGBM.gain_importance-Tuple{LGBMEstimator, Integer}"><code>LightGBM.gain_importance</code></a></li><li><a href="#LightGBM.loadmodel!-Tuple{LGBMEstimator, String}"><code>LightGBM.loadmodel!</code></a></li><li><a href="#LightGBM.predict-Union{Tuple{TX}, Tuple{LGBMEstimator, Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}}} where TX&lt;:Real"><code>LightGBM.predict</code></a></li><li><a href="#LightGBM.savemodel-Tuple{LGBMEstimator, String}"><code>LightGBM.savemodel</code></a></li><li><a href="#LightGBM.search_cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any, Any}} where {TX&lt;:Real, Ty&lt;:Real}"><code>LightGBM.search_cv</code></a></li><li><a href="#LightGBM.split_importance-Tuple{LGBMEstimator, Integer}"><code>LightGBM.split_importance</code></a></li></ul><article><details class="docstring" open="true"><summary id="LightGBM.LGBMClassification-Tuple{}"><a class="docstring-binding" href="#LightGBM.LGBMClassification-Tuple{}"><code>LightGBM.LGBMClassification</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">LGBMClassification(;[
    objective = &quot;multiclass&quot;,
    boosting = &quot;gbdt&quot;,
    num_iterations = 100,
    learning_rate = .1,
    num_leaves = 31,
    tree_learner = &quot;serial&quot;,
    num_threads = 0,
    device_type=&quot;cpu&quot;,
    seed = 0,
    deterministic = false,
    force_col_wise = false,
    force_row_wise = false,
    histogram_pool_size = -1.,
    max_depth = -1,
    min_data_in_leaf = 20,
    min_sum_hessian_in_leaf = 1e-3,
    bagging_fraction = 1.,
    pos_bagging_fraction = 1.,
    neg_bagging_fraction = 1.,
    bagging_freq = 0,
    bagging_seed = 3,
    feature_fraction = 1.,
    feature_fraction_bynode = 1.,
    feature_fraction_seed = 2,
    extra_trees = false,
    extra_seed = 6,
    early_stopping_round = 0,
    first_metric_only = false,
    max_delta_step = 0.,
    lambda_l1 = 0.,
    lambda_l2 = 0.,
    linear_lambda = 0.,
    min_gain_to_split = 0.,
    drop_rate = 0.1,
    max_drop = 50,
    skip_drop = 0.5,
    xgboost_dart_mode = false,
    uniform_drop = false,
    drop_seed = 4,
    top_rate = 0.2,
    other_rate = 0.1,
    min_data_per_group = 100,
    max_cat_threshold = 32,
    cat_l2 = 10.,
    cat_smooth = 10.,
    max_cat_to_onehot = 4,
    top_k = 20,
    monotone_constraints = Int[],
    monotone_constraints_method = &quot;basic&quot;,
    monotone_penalty = 0.,
    feature_contri = Float64[],
    forcedsplits_filename = &quot;&quot;,
    refit_decay_rate = 0.9,
    cegb_tradeoff = 1.0,
    cegb_penalty_split = 0.,
    cegb_penalty_feature_lazy = Float64[],
    cegb_penalty_feature_coupled = Float64[],
    path_smooth = 0.
    interaction_constraints = Vector{Int}[],  
    verbosity = 1,
    linear_tree = false,
    max_bin = 255,
    max_bin_by_feature = Int[],
    min_data_in_bin = 3,
    bin_construct_sample_cnt = 200000,
    data_random_seed = 1,
    is_enable_sparse = true,
    enable_bundle = true,
    use_missing = true,
    zero_as_missing = false,
    feature_pre_filter = true,
    pre_partition = false,
    two_round = false,
    header = false,
    label_column = &quot;&quot;,
    weight_column = &quot;&quot;,
    ignore_column = &quot;&quot;,
    categorical_feature = Int[],
    forcedbins_filename = &quot;&quot;,
    precise_float_parser = false,
    start_iteration_predict = 0,
    num_iteration_predict = -1,
    predict_raw_score = false,
    predict_leaf_index = false,
    predict_contrib = false,
    predict_disable_shape_check = false,
    pred_early_stop = false,
    pred_early_stop_freq = 10,
    pred_early_stop_margin = 10.0,
    num_class = 2,
    is_unbalance = false,
    scale_pos_weight = 1.0,
    sigmoid = 1.0,
    boost_from_average = true,
    metric = [&quot;&quot;],
    metric_freq = 1,
    is_provide_training_metric = false,
    eval_at = Int[1, 2, 3, 4, 5],
    multi_error_top_k = 1,
    auc_mu_weights = Float64[],
    num_machines = 1,
    local_listen_port = 12400,
    time_out = 120,
    machine_list_filename = &quot;&quot;,
    machines = &quot;&quot;,
    gpu_platform_id = -1,
    gpu_device_id = -1,
    gpu_use_dp = false,
    num_gpu = 1,
])</code></pre><p>Return a LGBMClassification estimator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/estimators.jl#LL498-L612">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.LGBMRanking-Tuple{}"><a class="docstring-binding" href="#LightGBM.LGBMRanking-Tuple{}"><code>LightGBM.LGBMRanking</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">LGBMRanking(;[
    objective = &quot;lambdarank&quot;,
    boosting = &quot;gbdt&quot;,
    num_iterations = 100,
    learning_rate = .1,
    num_leaves = 31,
    tree_learner = &quot;serial&quot;,
    num_threads = 0,
    device_type=&quot;cpu&quot;,
    seed = 0,
    deterministic = false,
    force_col_wise = false,
    force_row_wise = false,
    histogram_pool_size = -1.,
    max_depth = -1,
    min_data_in_leaf = 20,
    min_sum_hessian_in_leaf = 1e-3,
    bagging_fraction = 1.,
    pos_bagging_fraction = 1.,
    neg_bagging_fraction = 1.,
    bagging_freq = 0,
    bagging_seed = 3,
    feature_fraction = 1.,
    feature_fraction_bynode = 1.,
    feature_fraction_seed = 2,
    extra_trees = false,
    extra_seed = 6,
    early_stopping_round = 0,
    first_metric_only = false,
    max_delta_step = 0.,
    lambda_l1 = 0.,
    lambda_l2 = 0.,
    linear_lambda = 0.,
    min_gain_to_split = 0.,
    drop_rate = 0.1,
    max_drop = 50,
    skip_drop = 0.5,
    xgboost_dart_mode = false,
    uniform_drop = false,
    drop_seed = 4,
    top_rate = 0.2,
    other_rate = 0.1,
    min_data_per_group = 100,
    max_cat_threshold = 32,
    cat_l2 = 10.,
    cat_smooth = 10.,
    max_cat_to_onehot = 4,
    top_k = 20,
    monotone_constraints = Int[],
    monotone_constraints_method = &quot;basic&quot;,
    monotone_penalty = 0.,
    feature_contri = Float64[],
    forcedsplits_filename = &quot;&quot;,
    refit_decay_rate = 0.9,
    cegb_tradeoff = 1.0,
    cegb_penalty_split = 0.,
    cegb_penalty_feature_lazy = Float64[],
    cegb_penalty_feature_coupled = Float64[],
    path_smooth = 0.,
    interaction_constraints = Vector{Int}[],
    verbosity = 1,
    linear_tree = false,
    max_bin = 255,
    max_bin_by_feature = Int[],
    min_data_in_bin = 3,
    bin_construct_sample_cnt = 200000,
    data_random_seed = 1,
    is_enable_sparse = true,
    enable_bundle = true,
    use_missing = true,
    zero_as_missing = false,
    feature_pre_filter = true,
    pre_partition = false,
    two_round = false,
    header = false,
    label_column = &quot;&quot;,
    weight_column = &quot;&quot;,
    group_column = &quot;&quot;,
    ignore_column = &quot;&quot;,
    group_column = &quot;&quot;,
    categorical_feature = Int[],
    forcedbins_filename = &quot;&quot;,
    precise_float_parser = false,
    start_iteration_predict = 0,
    num_iteration_predict = -1,
    predict_raw_score = false,
    predict_leaf_index = false,
    predict_contrib = false,
    predict_disable_shape_check = false,
    pred_early_stop = false,
    pred_early_stop_freq = 10,
    pred_early_stop_margin = 10.0,
    objective_seed = 5,
    num_class = 1,
    is_unbalance = false,
    scale_pos_weight = 1.0,
    sigmoid = 1.0,
    boost_from_average = true,
    lambdarank_truncation_level = 30,
    lambdarank_norm = true,
    label_gain = [2^i - 1 for i in 0:30],
    metric = [&quot;&quot;],
    metric_freq = 1,
    is_provide_training_metric = false,
    eval_at = Int[1, 2, 3, 4, 5],
    num_machines = 1,
    local_listen_port = 12400,
    time_out = 120,
    machine_list_filename = &quot;&quot;,
    machines = &quot;&quot;,
    gpu_platform_id = -1,
    gpu_device_id = -1,
    gpu_use_dp = false,
    num_gpu = 1,
])</code></pre><p>Return a LGBMRanking estimator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/estimators.jl#LL878-L996">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.LGBMRegression-Tuple{}"><a class="docstring-binding" href="#LightGBM.LGBMRegression-Tuple{}"><code>LightGBM.LGBMRegression</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">LGBMRegression(; [
    objective = &quot;regression&quot;,
    boosting = &quot;gbdt&quot;,
    num_iterations = 100,
    learning_rate = .1,
    num_leaves = 31,
    tree_learner = &quot;serial&quot;,
    num_threads = 0,
    device_type=&quot;cpu&quot;,
    seed = 0,
    deterministic = false,
    force_col_wise = false
    force_row_wise = false
    histogram_pool_size = -1.,
    max_depth = -1,
    min_data_in_leaf = 20,
    min_sum_hessian_in_leaf = 1e-3,
    bagging_fraction = 1.,
    bagging_freq = 0,
    bagging_seed = 3,
    feature_fraction = 1.,
    feature_fraction_bynode = 1.,
    feature_fraction_seed = 2,
    extra_trees = false
    extra_seed = 6,
    early_stopping_round = 0,
    first_metric_only = false,
    max_delta_step = 0.,
    lambda_l1 = 0.,
    lambda_l2 = 0.,
    linear_lambda = 0.,
    min_gain_to_split = 0.,
    drop_rate = 0.1,
    max_drop = 50,
    skip_drop = 0.5,
    xgboost_dart_mode = false,
    uniform_drop = false,
    drop_seed = 4,
    top_rate = 0.2,
    other_rate = 0.1,
    min_data_per_group = 100,
    max_cat_threshold = 32,
    cat_l2 = 10.,
    cat_smooth = 10.,
    max_cat_to_onehot = 4,
    top_k = 20,
    monotone_constraints = Int[],
    monotone_constraints_method = &quot;basic&quot;,
    monotone_penalty = 0.,
    feature_contri = Float64[],
    forcedsplits_filename = &quot;&quot;,
    refit_decay_rate = 0.9,
    cegb_tradeoff = 1.0,
    cegb_penalty_split = 0.,
    cegb_penalty_feature_lazy = Float64[],
    cegb_penalty_feature_coupled = Float64[],
    path_smooth = 0.,
    interaction_constraints = Vector{Int}[],
    verbosity = 1,
    linear_tree = false,
    max_bin = 255,
    max_bin_by_feature = Int[],
    min_data_in_bin = 3
    bin_construct_sample_cnt = 200000,
    data_random_seed = 1,
    is_enable_sparse = true,
    enable_bundle = true,
    use_missing = true,
    zero_as_missing = false,
    feature_pre_filter = true,
    pre_partition = false,
    two_round = false,
    header = false,
    label_column = &quot;&quot;,
    weight_column = &quot;&quot;,
    ignore_column = &quot;&quot;,
    categorical_feature = Int[],
    forcedbins_filename = &quot;&quot;,
    precise_float_parser = false,
    start_iteration_predict = 0,
    num_iteration_predict = -1,
    predict_raw_score = false,
    predict_leaf_index = false,
    predict_contrib = false,
    predict_disable_shape_check = false,
    is_unbalance = false,
    boost_from_average = true,
    reg_sqrt = false,
    alpha = 0.9,
    fair_c = 1.0,
    poisson_max_delta_step = 0.7,
    tweedie_variance_power = 1.5,
    metric = [&quot;&quot;],
    metric_freq = 1,
    is_provide_training_metric = false,
    eval_at = Int[1, 2, 3, 4, 5],
    num_machines = 1,
    local_listen_port = 12400,
    time_out = 120,
    machine_list_filename = &quot;&quot;,
    machines = &quot;&quot;,
    gpu_platform_id = -1,
    gpu_device_id = -1,
    gpu_use_dp = false,
    num_gpu = 1,
])</code></pre><p>Return a LGBMRegression estimator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/estimators.jl#LL129-L238">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.LGBM_BoosterUpdateOneIterCustom-Tuple{LightGBM.Booster, Vector{&lt;:AbstractFloat}, Vector{&lt;:AbstractFloat}}"><a class="docstring-binding" href="#LightGBM.LGBM_BoosterUpdateOneIterCustom-Tuple{LightGBM.Booster, Vector{&lt;:AbstractFloat}, Vector{&lt;:AbstractFloat}}"><code>LightGBM.LGBM_BoosterUpdateOneIterCustom</code></a> — <span class="docstring-category">Method</span></summary><section><div><p>LGBM_BoosterUpdateOneIterCustom Pass grads and 2nd derivatives corresponding to some custom loss function grads and 2nd derivatives must be same cardinality as training data * number of models Also, trying to run this on a booster without data will fail.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/wrapper.jl#LL471-L476">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any}} where {TX&lt;:Real, Ty&lt;:Real}"><a class="docstring-binding" href="#LightGBM.cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any}} where {TX&lt;:Real, Ty&lt;:Real}"><code>LightGBM.cv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">cv(estimator, X, y, splits; [verbosity = 1])</code></pre><p>Cross-validate the <code>estimator</code> with features data <code>X</code> and label <code>y</code>. The iterable <code>splits</code> provides vectors of indices for the training dataset. The remaining indices are used to create the validation dataset. Alternatively, cv can be called with an input Dataset class</p><p>Return a dictionary with an entry for the validation dataset and, if the parameter <code>is_provide_training_metric</code> is set in the <code>estimator</code>, an entry for the training dataset. Each entry of the dictionary is another dictionary with an entry for each validation metric in the <code>estimator</code>. Each of these entries is an array that holds the validation metric&#39;s value for each dataset, at the last valid iteration.</p><p><strong>Arguments</strong></p><ul><li><code>estimator::LGBMEstimator</code>: the estimator to be fit.</li><li><code>X::Matrix{TX&lt;:Real}</code>: the features data.</li><li><code>y::Vector{Ty&lt;:Real}</code>: the labels.</li><li><code>dataset::Dataset</code>: prepared dataset (either (X, y), or dataset needs to be specified as input)</li><li><code>splits</code>: the iterable providing arrays of indices for the training dataset.</li><li><code>verbosity::Integer</code>: keyword argument that controls LightGBM&#39;s verbosity. <code>&lt; 0</code> for fatal logs   only, <code>0</code> includes warning logs, <code>1</code> includes info logs, and <code>&gt; 1</code> includes debug logs.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/cv.jl#LL1-L24">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.fit!-Union{Tuple{Ti}, Tuple{Tw}, Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}, Vector{Ty}, Vararg{Tuple{Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}, Vector{Ty}}}}} where {TX&lt;:Real, Ty&lt;:Real, Tw&lt;:Real, Ti&lt;:Real}"><a class="docstring-binding" href="#LightGBM.fit!-Union{Tuple{Ti}, Tuple{Tw}, Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}, Vector{Ty}, Vararg{Tuple{Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}, Vector{Ty}}}}} where {TX&lt;:Real, Ty&lt;:Real, Tw&lt;:Real, Ti&lt;:Real}"><code>LightGBM.fit!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">fit!(
estimator::LGBMEstimator, 
X::Union{AbstractMatrix{TX}, AbstractMatrix{Union{TX, Missing}}}, 
y::Vector{Ty}, 
test::Tuple{Union{AbstractMatrix{TX}, AbstractMatrix{Union{TX, Missing}}}, Vector{Ty}}...;
verbosity::Integer = 1,
is_row_major = false,
weights::Vector{Tw} = Float32[],
init_score::Vector{Ti} = Float64[],
group::Vector{Int} = Int[],
truncate_booster::Bool=true,</code></pre><p>) where {TX&lt;:Real,Ty&lt;:Real,Tw&lt;:Real,Ti&lt;:Real}     fit!(estimator, X, y[, test...]; [verbosity = 1, is<em>row</em>major = false])     fit!(estimator, X, y, train<em>indices[, test</em>indices...]; [verbosity = 1, is<em>row</em>major = false])     fit!(estimator, train<em>dataset[, test</em>datasets...]; [verbosity = 1])</p><p>Fit the <code>estimator</code> with features data <code>X</code> and label <code>y</code> using the X-y pairs in <code>test</code> as validation sets. Alternatively, Fit the <code>estimator</code> with <code>train_dataset</code> and <code>test_datasets</code> in the form of Dataset class(es)</p><p>Return a dictionary with an entry for each validation set. Each entry of the dictionary is another dictionary with an entry for each validation metric in the <code>estimator</code>. Each of these entries is an array that holds the validation metric&#39;s value at each iteration.</p><p><strong>Positional Arguments</strong></p><ul><li><code>estimator::LGBMEstimator</code>: the estimator to be fit.</li><li>and either<ul><li><code>X::Union{AbstractMatrix{TX}, AbstractMatrix{Union{TX, Missing}}}</code>: the features data. May be a <code>SparseArrays.SparseMatrixCSC</code></li></ul>If <code>X</code> is of type <code>Union{Float, Missing}</code>, missing values will be replaced with <code>NaN</code>.   If <code>X</code> is of type <code>Int</code>, missing values will be replaced with <code>NaN</code> after casting to <code>Float64</code>.<ul><li><code>y::Vector{Ty&lt;:Real}</code>: the labels.</li><li><code>test::Tuple{Union{AbstractMatrix{TX}, AbstractMatrix{Union{TX, Missing}}}, Vector{Ty}}...</code>: (optional)    contains one or more tuples of X-y pairs of the same types as <code>X</code> and <code>y</code> that should be used as validation sets.    May be a <code>SparseArrays.SparseMatrixCSC</code> and contain missing values.    Supports mix-and-match sparse/dense among these test and the train.</li></ul></li><li>or<ul><li><code>train_dataset::Dataset</code>: prepared train_dataset</li><li><code>test_datasets::Vector{Dataset}</code>: (optional) prepared test_datasets</li></ul></li><li>or<ul><li><code>train_filepath::String</code>: path to the training data file.</li><li><code>test_filepath::String</code>: (optional) path to the test data file.</li></ul></li></ul><p><strong>Keyword Arguments</strong></p><ul><li><code>verbosity::Integer</code>: keyword argument that controls LightGBM&#39;s verbosity. <code>&lt; 0</code> for fatal logs   only, <code>0</code> includes warning logs, <code>1</code> includes info logs, and <code>&gt; 1</code> includes debug logs.</li><li><code>is_row_major::Bool</code>: keyword argument that indicates whether or not <code>X</code> is row-major. <code>true</code>   indicates that it is row-major, <code>false</code> indicates that it is column-major (Julia&#39;s default).   Should be consistent across train/test. Does not apply to <code>SparseArrays.SparseMatrixCSC</code> or <code>Dataset</code> constructors.</li><li><code>weights::Vector{Tw&lt;:Real}</code>: the training weights.</li><li><code>init_score::Vector{Ti&lt;:Real}</code>: the init scores.</li><li><code>group::Vector{Int}</code>: group size information for ranking tasks.</li><li><code>truncate_booster::Bool</code>: allows to reduce the size of the model by removing less impactful trees. Default is <code>true</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/fit.jl#LL1-L53">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.gain_importance-Tuple{LGBMEstimator, Integer}"><a class="docstring-binding" href="#LightGBM.gain_importance-Tuple{LGBMEstimator, Integer}"><code>LightGBM.gain_importance</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">gain_importance(estimator, num_iteration)
gain_importance(estimator)

Returns the importance of a fitted booster in terms of information gain across
all boostings, or up to `num_iteration` boostings</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/utils.jl#LL103-L109">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.loadmodel!-Tuple{LGBMEstimator, String}"><a class="docstring-binding" href="#LightGBM.loadmodel!-Tuple{LGBMEstimator, String}"><code>LightGBM.loadmodel!</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">loadmodel!(estimator, filename)</code></pre><p>Load the fitted model <code>filename</code> into <code>estimator</code>. Note that this only loads the fitted model—not the parameters or data of the estimator whose model was saved as <code>filename</code>.</p><p><strong>Arguments</strong></p><ul><li><code>estimator::LGBMEstimator</code>: the estimator to use in the prediction.</li><li><code>filename::String</code>: the name of the file that contains the model.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/utils.jl#LL50-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.predict-Union{Tuple{TX}, Tuple{LGBMEstimator, Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}}} where TX&lt;:Real"><a class="docstring-binding" href="#LightGBM.predict-Union{Tuple{TX}, Tuple{LGBMEstimator, Union{AbstractMatrix{TX}, AbstractArray{Union{Missing, TX}, 2}}}} where TX&lt;:Real"><code>LightGBM.predict</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">predict(estimator, X; [num_iterations = -1, verbosity = 1,
is_row_major = false, num_threads = -1, predict_raw_score = false, predict_leaf_index = false, predict_contrib = false])</code></pre><p>Return a <strong>MATRIX</strong> with the labels that the <code>estimator</code> predicts for features data <code>X</code>. Use <code>dropdims</code> if a vector is required.</p><p><strong>Arguments</strong></p><ul><li><code>estimator::LGBMEstimator</code>: the estimator to use in the prediction.</li><li><code>X::Union{AbstractMatrix{TX}, AbstractMatrix{Union{TX, Missing}}}</code>: the features data.   If <code>X</code> is of type <code>Union{Float, Missing}</code>, missing values will be replaced with <code>NaN</code>.   If <code>X</code> is of type <code>Int</code>, missing values will be replaced with <code>NaN</code> after casting to <code>Float64</code>.</li><li><code>predict_type::Integer</code>: keyword argument that controls the prediction type. <code>0</code> for normal   scores with transform (if needed), <code>1</code> for raw scores, <code>2</code> for leaf indices, <code>3</code> for SHAP contributions.</li><li><code>num_iterations::Integer</code>: keyword argument that sets the number of iterations of the model to   use in the prediction. <code>&lt; 0</code> for all iterations.</li><li><code>verbosity::Integer</code>: keyword argument that controls LightGBM&#39;s verbosity. <code>&lt; 0</code> for fatal logs   only, <code>0</code> includes warning logs, <code>1</code> includes info logs, and <code>&gt; 1</code> includes debug logs.</li><li><code>is_row_major::Bool</code>: keyword argument that indicates whether or not <code>X</code> is row-major. <code>true</code>   indicates that it is row-major, <code>false</code> indicates that it is column-major (Julia&#39;s default).</li><li><code>num_threads::Integer</code>: keyword argument specifying the number of threads to use   for prediction. Default is <code>-1</code> which reuses <code>num_threads</code> of the estimator.</li><li><code>predict_raw_score::Bool</code>: keyword argument that indicates whether or not to predict raw scores. Setting to true overrides the estimator&#39;s setting.</li><li><code>predict_leaf_index::Bool</code>: keyword argument that indicates whether or not to predict leaf indices. Setting to true overrides the estimator&#39;s setting.</li><li><code>predict_contrib::Bool</code>: keyword argument that indicates whether or not to predict SHAP contributions. Setting to true overrides the estimator&#39;s setting.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/predict.jl#LL33-L59">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.savemodel-Tuple{LGBMEstimator, String}"><a class="docstring-binding" href="#LightGBM.savemodel-Tuple{LGBMEstimator, String}"><code>LightGBM.savemodel</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">savemodel(estimator, filename; [num_iteration = -1])</code></pre><p>Save the fitted model in <code>estimator</code> as <code>filename</code>.</p><p><strong>Arguments</strong></p><ul><li><code>estimator::LGBMEstimator</code>: the estimator to use in the prediction.</li><li><code>filename::String</code>: the name of the file to save the model in.</li><li><code>num_iteration::Integer</code>: keyword argument that sets the number of iterations of the model that   should be saved. <code>&lt; 0</code> for all iterations.</li><li><code>start_iteration</code> : : Start index of the iteration that should be saved.</li><li><code>feature_importance_type</code> : Type of feature importance,   can be C<em>API</em>FEATURE<em>IMPORTANCE</em>SPLIT or C<em>API</em>FEATURE<em>IMPORTANCE</em>GAIN</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/utils.jl#LL24-L37">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.search_cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any, Any}} where {TX&lt;:Real, Ty&lt;:Real}"><a class="docstring-binding" href="#LightGBM.search_cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any, Any}} where {TX&lt;:Real, Ty&lt;:Real}"><code>LightGBM.search_cv</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">search_cv(estimator, X, y, splits, params; [verbosity = 1])</code></pre><p>Exhaustive search over the specified sets of parameter values for the <code>estimator</code> with features data <code>X</code> and label <code>y</code>. The iterable <code>splits</code> provides vectors of indices for the training dataset. The remaining indices are used to create the validation dataset. Alternatively, search_cv can be called with an input Dataset class</p><p>Return an array with a tuple for each set of parameters value, where the first entry is a set of parameter values and the second entry the cross-validation outcome of those values. This outcome is a dictionary with an entry for the validation dataset and, if the parameter <code>is_provide_training_metric</code> is set in the <code>estimator</code>, an entry for the training dataset. Each entry of the dictionary is another dictionary with an entry for each validation metric in the <code>estimator</code>. Each of these entries is an array that holds the validation metric&#39;s value for each dataset, at the last valid iteration.</p><p><strong>Arguments</strong></p><ul><li><code>estimator::LGBMEstimator</code>: the estimator to be fit.</li><li><code>X::Matrix{TX&lt;:Real}</code>: the features data.</li><li><code>y::Vector{Ty&lt;:Real}</code>: the labels.</li><li><code>dataset::Dataset</code>: prepared dataset (either (X, y), or dataset needs to be specified as input)</li><li><code>splits</code>: the iterable providing arrays of indices for the training dataset.</li><li><code>params</code>: the iterable providing dictionaries of pairs of parameters (Symbols) and values to   configure the <code>estimator</code> with.</li><li><code>verbosity::Integer</code>: keyword argument that controls LightGBM&#39;s verbosity. <code>&lt; 0</code> for fatal logs   only, <code>0</code> includes warning logs, <code>1</code> includes info logs, and <code>&gt; 1</code> includes debug logs.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/search_cv.jl#LL1-L28">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.split_importance-Tuple{LGBMEstimator, Integer}"><a class="docstring-binding" href="#LightGBM.split_importance-Tuple{LGBMEstimator, Integer}"><code>LightGBM.split_importance</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">split_importance(estimator, num_iteration)
split_importance(estimator)

Returns the importance of a fitted booster in terms of number of times feature was
used in a split across all boostings, or up to `num_iteration` boostings</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/utils.jl#LL114-L120">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.MLJInterface.LGBMClassifier"><a class="docstring-binding" href="#LightGBM.MLJInterface.LGBMClassifier"><code>LightGBM.MLJInterface.LGBMClassifier</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">LGBMClassifier</code></pre><p>A model type for constructing a LightGBM classifier, based on <a href="https://github.com/IQVIA-ML/LightGBM.jl">LightGBM.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-julia hljs">LGBMClassifier = @load LGBMClassifier pkg=LightGBM</code></pre><p>Do <code>model = LGBMClassifier()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>LGBMClassifier(objective=...)</code>.</p><p>`LightGBM, short for light gradient-boosting machine, is a framework for gradient boosting based on decision tree algorithms and used for classification and other machine learning tasks, with a focus on performance and scalability. This model in particular is used for various types of classification tasks.</p><p><strong>Training data In MLJ or MLJBase, bind an instance <code>model</code> to data with</strong></p><p>mach = machine(model, X, y) </p><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check the column scitypes with <code>schema(X)</code>; alternatively, <code>X</code> is any <code>AbstractMatrix</code> with <code>Continuous</code> elements; check the scitype with <code>scitype(X)</code>.</li><li>y is a vector of targets whose items are of scitype <code>Continuous</code>. Check the scitype with scitype(y).</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above.</li></ul><p><strong>Hyper-parameters</strong></p><p>See https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html.</p><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>fitresult</code>: Fitted model information, contains a <code>LGBMClassification</code> object, a <code>CategoricalArray</code> of the input class names, and the classifier with all its parameters</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_metrics</code>: A dictionary containing all training metrics.</li><li><code>importance</code>: A <code>namedtuple</code> containing:<ul><li><code>gain</code>: The total gain of each split used by the model</li><li><code>split</code>: The number of times each feature is used by the model.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">
using DataFrames
using MLJ

# load the model
LGBMClassifier = @load LGBMClassifier pkg=LightGBM 

X, y = @load_iris 
X = DataFrame(X)
train, test = partition(collect(eachindex(y)), 0.70, shuffle=true)

first(X, 3)
lgb = LGBMClassifier() # initialise a model with default params
mach = machine(lgb, X[train, :], y[train]) |&gt; fit!

predict(mach, X[test, :])

# access feature importances via MLJ interface (recommended)
importances = feature_importances(mach)  # vector of feature_name =&gt; importance pairs

# or access directly from report
model_report = report(mach)
gain_importance = model_report.importance.gain   # raw importance vectors
split_importance = model_report.importance.split</code></pre><p><strong>Note</strong>: The <code>feature_importance</code> hyperparameter (<code>:gain</code> or <code>:split</code>) controls which  importance method is used by <code>feature_importances()</code>. Default is <code>:gain</code>.</p><p>See also <a href="https://github.com/IQVIA-ML/LightGBM.jl">LightGBM.jl</a> and the unwrapped model type <a href="#LightGBM.LGBMClassification-Tuple{}"><code>LightGBM.LGBMClassification</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/docstrings.jl#LL103-L118">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LightGBM.MLJInterface.LGBMRegressor"><a class="docstring-binding" href="#LightGBM.MLJInterface.LGBMRegressor"><code>LightGBM.MLJInterface.LGBMRegressor</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">LGBMRegressor</code></pre><p>A model type for constructing a LightGBM regressor, based on <a href="https://github.com/IQVIA-ML/LightGBM.jl">LightGBM.jl</a>, and implementing the MLJ model interface.</p><p>From MLJ, the type can be imported using</p><pre><code class="language-julia hljs">LGBMRegressor = @load LGBMRegressor pkg=LightGBM</code></pre><p>Do <code>model = LGBMRegressor()</code> to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in <code>LGBMRegressor(objective=...)</code>.</p><p>LightGBM, short for light gradient-boosting machine, is a framework for gradient boosting based on decision tree algorithms and used for classification, regression and other machine learning tasks, with a focus on performance and scalability. This model in particular is used for various types of regression tasks.</p><p><strong>Training data</strong></p><p>In MLJ or MLJBase, bind an instance <code>model</code> to data with </p><p>mach = machine(model, X, y) </p><p>Here:</p><ul><li><code>X</code> is any table of input features (eg, a <code>DataFrame</code>) whose columns are of scitype <code>Continuous</code>; check the column scitypes with <code>schema(X)</code>; alternatively, <code>X</code> is any <code>AbstractMatrix</code> with <code>Continuous</code> elements; check the scitype with <code>scitype(X)</code>.</li><li>y is a vector of targets whose items are of scitype <code>Continuous</code>. Check the scitype with <code>scitype(y)</code>.</li></ul><p>Train the machine using <code>fit!(mach, rows=...)</code>.</p><p><strong>Operations</strong></p><ul><li><code>predict(mach, Xnew)</code>: return predictions of the target given new features <code>Xnew</code>, which should have the same scitype as <code>X</code> above.</li></ul><p><strong>Hyper-parameters</strong></p><p>See https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html.</p><p><strong>Fitted parameters</strong></p><p>The fields of <code>fitted_params(mach)</code> are:</p><ul><li><code>fitresult</code>: Fitted model information, contains a <code>LGBMRegression</code> object, an empty vector, and the regressor with all its parameters</li></ul><p><strong>Report</strong></p><p>The fields of <code>report(mach)</code> are:</p><ul><li><code>training_metrics</code>: A dictionary containing all training metrics.</li><li><code>importance</code>: A <code>namedtuple</code> containing:<ul><li><code>gain</code>: The total gain of each split used by the model</li><li><code>split</code>: The number of times each feature is used by the model.</li></ul></li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs">
using DataFrames
using MLJ

# load the model
LGBMRegressor = @load LGBMRegressor pkg=LightGBM 

X, y = @load_boston # a table and a vector 
X = DataFrame(X)
train, test = partition(collect(eachindex(y)), 0.70, shuffle=true)

first(X, 3)
lgb = LGBMRegressor() # initialise a model with default params
mach = machine(lgb, X[train, :], y[train]) |&gt; fit!

predict(mach, X[test, :])

# access feature importances via MLJ interface (recommended)
importances = feature_importances(mach)  # vector of feature_name =&gt; importance pairs

# or access directly from report
model_report = report(mach)
gain_importance = model_report.importance.gain   # raw importance vectors
split_importance = model_report.importance.split</code></pre><p><strong>Note</strong>: The <code>feature_importance</code> hyperparameter (<code>:gain</code> or <code>:split</code>) controls which  importance method is used by <code>feature_importances()</code>. Default is <code>:gain</code>.</p><p>See also <a href="https://github.com/IQVIA-ML/LightGBM.jl">LightGBM.jl</a> and the unwrapped model type <a href="#LightGBM.LGBMRegression-Tuple{}"><code>LightGBM.LGBMRegression</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/IQVIA-ML/LightGBM.jl/blob/9992a803d4c788589892d3e06ecb5b2afb37dd35/src/docstrings.jl#LL13-L28">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../">« Home</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 12 January 2026 15:16">Monday 12 January 2026</span>. Using Julia version 1.12.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
