var documenterSearchIndex = {"docs":
[{"location":"functions/#Functions","page":"Functions","title":"Functions","text":"","category":"section"},{"location":"functions/","page":"Functions","title":"Functions","text":"","category":"page"},{"location":"functions/","page":"Functions","title":"Functions","text":"Modules = [LightGBM, LightGBM.MLJInterface]","category":"page"},{"location":"functions/#LightGBM.LGBMClassification-Tuple{}","page":"Functions","title":"LightGBM.LGBMClassification","text":"LGBMClassification(;[\n    objective = \"multiclass\",\n    boosting = \"gbdt\",\n    num_iterations = 100,\n    learning_rate = .1,\n    num_leaves = 31,\n    max_depth = -1,\n    tree_learner = \"serial\",\n    num_threads = 0,\n    histogram_pool_size = -1.,\n    min_data_in_leaf = 20,\n    min_sum_hessian_in_leaf = 1e-3,\n    max_delta_step = 0.,\n    lambda_l1 = 0.,\n    lambda_l2 = 0.,\n    min_gain_to_split = 0.,\n    feature_fraction = 1.,\n    feature_fraction_bynode = 1.,\n    feature_fraction_seed = 2,\n    bagging_fraction = 1.,\n    pos_bagging_fraction = 1.,\n    neg_bagging_fraction = 1.,\n    bagging_freq = 0,\n    bagging_seed = 3,\n    early_stopping_round = 0,\n    extra_trees = false,\n    extra_seed = 6,\n    max_bin = 255,\n    bin_construct_sample_cnt = 200000,\n    data_random_seed = 1,\n    is_enable_sparse = true,\n    save_binary = false,\n    categorical_feature = Int[],\n    use_missing = true,\n    linear_tree = false,\n    feature_pre_filter = true,\n    is_unbalance = false,\n    boost_from_average = true,\n    scale_pos_weight = 1.0,\n    sigmoid = 1.0,\n    drop_rate = 0.1,\n    max_drop = 50,\n    skip_drop = 0.5,\n    xgboost_dart_mode = false,\n    uniform_drop = false,\n    drop_seed = 4,\n    top_rate = 0.2,\n    other_rate = 0.1,\n    min_data_per_group = 100,\n    max_cat_threshold = 32,\n    cat_l2 = 10.0,\n    cat_smooth = 10.0,\n    metric = [\"\"],\n    metric_freq = 1,\n    is_provide_training_metric = false,\n    eval_at = Int[1, 2, 3, 4, 5],\n    num_machines = 1,\n    local_listen_port = 12400,\n    time_out = 120,\n    machine_list_filename = \"\",\n    num_class = 2,\n    device_type=\"cpu\",\n    gpu_use_dp = false,\n    gpu_platform_id = -1,\n    gpu_device_id = -1,\n    num_gpu = 1,\n    force_col_wise = false,\n    force_row_wise = false,\n])\n\nReturn a LGBMClassification estimator.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.LGBMRanking-Tuple{}","page":"Functions","title":"LightGBM.LGBMRanking","text":"LGBMRanking(;[\n    objective = \"lambdarank\",\n    boosting = \"gbdt\",\n    num_iterations = 100,\n    learning_rate = .1,\n    num_leaves = 31,\n    max_depth = -1,\n    tree_learner = \"serial\",\n    num_threads = 0,\n    histogram_pool_size = -1.,\n    min_data_in_leaf = 20,\n    min_sum_hessian_in_leaf = 1e-3,\n    max_delta_step = 0.,\n    lambda_l1 = 0.,\n    lambda_l2 = 0.,\n    min_gain_to_split = 0.,\n    feature_fraction = 1.,\n    feature_fraction_bynode = 1.,\n    feature_fraction_seed = 2,\n    bagging_fraction = 1.,\n    pos_bagging_fraction = 1.,\n    neg_bagging_fraction = 1.,\n    bagging_freq = 0,\n    bagging_seed = 3,\n    early_stopping_round = 0,\n    extra_trees = false,\n    extra_seed = 6,\n    max_bin = 255,\n    bin_construct_sample_cnt = 200000,\n    data_random_seed = 1,\n    is_enable_sparse = true,\n    save_binary = false,\n    categorical_feature = Int[],\n    use_missing = true,\n    linear_tree = false,\n    feature_pre_filter = true,\n    is_unbalance = false,\n    boost_from_average = true,\n    scale_pos_weight = 1.0,\n    sigmoid = 1.0,\n    drop_rate = 0.1,\n    max_drop = 50,\n    skip_drop = 0.5,\n    xgboost_dart_mode = false,\n    uniform_drop = false,\n    drop_seed = 4,\n    top_rate = 0.2,\n    other_rate = 0.1,\n    min_data_per_group = 100,\n    max_cat_threshold = 32,\n    cat_l2 = 10.0,\n    cat_smooth = 10.0,\n    metric = [\"\"],\n    metric_freq = 1,\n    is_provide_training_metric = false,\n    eval_at = Int[1, 2, 3, 4, 5],\n    num_machines = 1,\n    local_listen_port = 12400,\n    time_out = 120,\n    machine_list_filename = \"\",\n    num_class = 1,\n    device_type=\"cpu\",\n    gpu_use_dp = false,\n    gpu_platform_id = -1,\n    gpu_device_id = -1,\n    num_gpu = 1,\n    force_col_wise = false,\n    force_row_wise = false,\n    lambdarank_truncation_level = 30,\n    lambdarank_norm = true,\n    label_gain = [2^i - 1 for i in 0:30],\n    objective_seed = 5,\n    group_column = \"\"\n])\n\nReturn a LGBMRanking estimator.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.LGBMRegression-Tuple{}","page":"Functions","title":"LightGBM.LGBMRegression","text":"LGBMRegression(; [\n    objective = \"regression\",\n    boosting = \"gbdt\",\n    num_iterations = 100,\n    learning_rate = .1,\n    num_leaves = 31,\n    max_depth = -1,\n    tree_learner = \"serial\",\n    num_threads = 0,\n    histogram_pool_size = -1.,\n    min_data_in_leaf = 20,\n    min_sum_hessian_in_leaf = 1e-3,\n    max_delta_step = 0.,\n    lambda_l1 = 0.,\n    lambda_l2 = 0.,\n    min_gain_to_split = 0.,\n    feature_fraction = 1.,\n    feature_fraction_bynode = 1.,\n    feature_fraction_seed = 2,\n    bagging_fraction = 1.,\n    bagging_freq = 0,\n    bagging_seed = 3,\n    early_stopping_round = 0,\n    extra_trees = false\n    extra_seed = 6,\n    max_bin = 255,\n    bin_construct_sample_cnt = 200000,\n    data_random_seed = 1,\n    is_enable_sparse = true,\n    save_binary = false,\n    categorical_feature = Int[],\n    use_missing = true,\n    linear_tree = false,\n    feature_pre_filter = true,\n    is_unbalance = false,\n    boost_from_average = true,\n    alpha = 0.9,\n    drop_rate = 0.1,\n    max_drop = 50,\n    skip_drop = 0.5,\n    xgboost_dart_mode = false,\n    uniform_drop = false,\n    drop_seed = 4,\n    top_rate = 0.2,\n    other_rate = 0.1,\n    min_data_per_group = 100,\n    max_cat_threshold = 32,\n    cat_l2 = 10.0,\n    cat_smooth = 10.0,\n    metric = [\"\"],\n    metric_freq = 1,\n    is_provide_training_metric = false,\n    eval_at = Int[1, 2, 3, 4, 5],\n    num_machines = 1,\n    local_listen_port = 12400,\n    time_out = 120,\n    machine_list_filename = \"\",\n    device_type=\"cpu\",\n    gpu_use_dp = false,\n    gpu_platform_id = -1,\n    gpu_device_id = -1,\n    num_gpu = 1,\n    force_col_wise = false\n    force_row_wise = false\n])\n\nReturn a LGBMRegression estimator.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.LGBM_BoosterUpdateOneIterCustom-Tuple{LightGBM.Booster, Vector{<:AbstractFloat}, Vector{<:AbstractFloat}}","page":"Functions","title":"LightGBM.LGBM_BoosterUpdateOneIterCustom","text":"LGBM_BoosterUpdateOneIterCustom Pass grads and 2nd derivatives corresponding to some custom loss function grads and 2nd derivatives must be same cardinality as training data * number of models Also, trying to run this on a booster without data will fail.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any}} where {TX<:Real, Ty<:Real}","page":"Functions","title":"LightGBM.cv","text":"cv(estimator, X, y, splits; [verbosity = 1])\n\nCross-validate the estimator with features data X and label y. The iterable splits provides vectors of indices for the training dataset. The remaining indices are used to create the validation dataset. Alternatively, cv can be called with an input Dataset class\n\nReturn a dictionary with an entry for the validation dataset and, if the parameter is_provide_training_metric is set in the estimator, an entry for the training dataset. Each entry of the dictionary is another dictionary with an entry for each validation metric in the estimator. Each of these entries is an array that holds the validation metric's value for each dataset, at the last valid iteration.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to be fit.\nX::Matrix{TX<:Real}: the features data.\ny::Vector{Ty<:Real}: the labels.\ndataset::Dataset: prepared dataset (either (X, y), or dataset needs to be specified as input)\nsplits: the iterable providing arrays of indices for the training dataset.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.fit!-Union{Tuple{Ti}, Tuple{Tw}, Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, AbstractMatrix{TX}, Vector{Ty}, Vararg{Tuple{AbstractMatrix{TX}, Vector{Ty}}}}} where {TX<:Real, Ty<:Real, Tw<:Real, Ti<:Real}","page":"Functions","title":"LightGBM.fit!","text":"fit!(\nestimator::LGBMEstimator, X::AbstractMatrix{TX}, y::Vector{Ty}, test::Tuple{AbstractMatrix{TX},Vector{Ty}}...;\nverbosity::Integer = 1,\nis_row_major = false,\nweights::Vector{Tw} = Float32[],\ninit_score::Vector{Ti} = Float64[],\ngroup::Vector{Int} = Int[],\ntruncate_booster::Bool=true,\n\n) where {TX<:Real,Ty<:Real,Tw<:Real,Ti<:Real}     fit!(estimator, X, y[, test...]; [verbosity = 1, isrowmajor = false])     fit!(estimator, X, y, trainindices[, testindices...]; [verbosity = 1, isrowmajor = false])     fit!(estimator, traindataset[, testdatasets...]; [verbosity = 1])\n\nFit the estimator with features data X and label y using the X-y pairs in test as validation sets. Alternatively, Fit the estimator with train_dataset and test_datasets in the form of Dataset class(es)\n\nReturn a dictionary with an entry for each validation set. Each entry of the dictionary is another dictionary with an entry for each validation metric in the estimator. Each of these entries is an array that holds the validation metric's value at each iteration.\n\nPositional Arguments\n\nestimator::LGBMEstimator: the estimator to be fit.\nand either\nX::AbstractMatrix{TX<:Real}: the features data. May be a SparseArrays.SparseMatrixCSC\ny::Vector{Ty<:Real}: the labels.\ntest::Tuple{AbstractMatrix{TX},Vector{Ty}}...: (optional) contains one or more tuples of X-y pairs of   the same types as X and y that should be used as validation sets. May be a SparseArrays.SparseMatrixCSC   and can mix-and-match sparse/dense among these test and the train.\nor\ntrain_dataset::Dataset: prepared train_dataset\ntest_datasets::Vector{Dataset}: (optional) prepared test_datasets\n\nKeyword Arguments\n\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\nis_row_major::Bool: keyword argument that indicates whether or not X is row-major. true   indicates that it is row-major, false indicates that it is column-major (Julia's default).   Should be consistent across train/test. Does not apply to SparseArrays.SparseMatrixCSC or Dataset constructors.\nweights::Vector{Tw<:Real}: the training weights.\ninit_score::Vector{Ti<:Real}: the init scores.\ngroup::Vector{Int}: group size information for ranking tasks.\ntruncate_booster::Bool: allows to reduce the size of the model by removing less impactful trees. Default is true.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.gain_importance-Tuple{LGBMEstimator, Integer}","page":"Functions","title":"LightGBM.gain_importance","text":"gain_importance(estimator, num_iteration)\ngain_importance(estimator)\n\nReturns the importance of a fitted booster in terms of information gain across\nall boostings, or up to `num_iteration` boostings\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.loadmodel!-Tuple{LGBMEstimator, String}","page":"Functions","title":"LightGBM.loadmodel!","text":"loadmodel!(estimator, filename)\n\nLoad the fitted model filename into estimator. Note that this only loads the fitted model—not the parameters or data of the estimator whose model was saved as filename.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nfilename::String: the name of the file that contains the model.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.predict-Union{Tuple{TX}, Tuple{LGBMEstimator, AbstractMatrix{TX}}} where TX<:Real","page":"Functions","title":"LightGBM.predict","text":"predict(estimator, X; [predict_type = 0, num_iterations = -1, verbosity = 1,\nis_row_major = false])\n\nReturn a MATRIX with the labels that the estimator predicts for features data X. Use dropdims if a vector is required.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nX::Matrix{T<:Real}: the features data.\npredict_type::Integer: keyword argument that controls the prediction type. 0 for normal   scores with transform (if needed), 1 for raw scores, 2 for leaf indices, 3 for SHAP contributions.\nnum_iterations::Integer: keyword argument that sets the number of iterations of the model to   use in the prediction. < 0 for all iterations.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\nis_row_major::Bool: keyword argument that indicates whether or not X is row-major. true   indicates that it is row-major, false indicates that it is column-major (Julia's default).\nnum_threads::Integer: keyword argument specifying the number of threads to use   for prediction. Default is -1 which reuses num_threads of the estimator.\n\nOne can obtain some form of feature importances by averaging SHAP contributions across predictions, i.e. mean(LightGBM.predict(estimator, X; predict_type=3); dims=1)\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.savemodel-Tuple{LGBMEstimator, String}","page":"Functions","title":"LightGBM.savemodel","text":"savemodel(estimator, filename; [num_iteration = -1])\n\nSave the fitted model in estimator as filename.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to use in the prediction.\nfilename::String: the name of the file to save the model in.\nnum_iteration::Integer: keyword argument that sets the number of iterations of the model that   should be saved. < 0 for all iterations.\nstart_iteration : : Start index of the iteration that should be saved.\nfeature_importance_type : Type of feature importance,   can be CAPIFEATUREIMPORTANCESPLIT or CAPIFEATUREIMPORTANCEGAIN\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.search_cv-Union{Tuple{Ty}, Tuple{TX}, Tuple{LGBMEstimator, Matrix{TX}, Vector{Ty}, Any, Any}} where {TX<:Real, Ty<:Real}","page":"Functions","title":"LightGBM.search_cv","text":"search_cv(estimator, X, y, splits, params; [verbosity = 1])\n\nExhaustive search over the specified sets of parameter values for the estimator with features data X and label y. The iterable splits provides vectors of indices for the training dataset. The remaining indices are used to create the validation dataset. Alternatively, search_cv can be called with an input Dataset class\n\nReturn an array with a tuple for each set of parameters value, where the first entry is a set of parameter values and the second entry the cross-validation outcome of those values. This outcome is a dictionary with an entry for the validation dataset and, if the parameter is_provide_training_metric is set in the estimator, an entry for the training dataset. Each entry of the dictionary is another dictionary with an entry for each validation metric in the estimator. Each of these entries is an array that holds the validation metric's value for each dataset, at the last valid iteration.\n\nArguments\n\nestimator::LGBMEstimator: the estimator to be fit.\nX::Matrix{TX<:Real}: the features data.\ny::Vector{Ty<:Real}: the labels.\ndataset::Dataset: prepared dataset (either (X, y), or dataset needs to be specified as input)\nsplits: the iterable providing arrays of indices for the training dataset.\nparams: the iterable providing dictionaries of pairs of parameters (Symbols) and values to   configure the estimator with.\nverbosity::Integer: keyword argument that controls LightGBM's verbosity. < 0 for fatal logs   only, 0 includes warning logs, 1 includes info logs, and > 1 includes debug logs.\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.split_importance-Tuple{LGBMEstimator, Integer}","page":"Functions","title":"LightGBM.split_importance","text":"split_importance(estimator, num_iteration)\nsplit_importance(estimator)\n\nReturns the importance of a fitted booster in terms of number of times feature was\nused in a split across all boostings, or up to `num_iteration` boostings\n\n\n\n\n\n","category":"method"},{"location":"functions/#LightGBM.MLJInterface.LGBMClassifier","page":"Functions","title":"LightGBM.MLJInterface.LGBMClassifier","text":"LGBMClassifier\n\nA model type for constructing a LightGBM classifier, based on LightGBM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLGBMClassifier = @load LGBMClassifier pkg=LightGBM\n\nDo model = LGBMClassifier() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LGBMClassifier(boosting=...).\n\n`LightGBM, short for light gradient-boosting machine, is a framework for gradient boosting based on decision tree algorithms and used for classification and other machine learning tasks, with a focus on performance and scalability. This model in particular is used for various types of classification tasks.\n\nTraining data In MLJ or MLJBase, bind an instance model to data with\n\nmach = machine(model, X, y) \n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the column scitypes with schema(X); alternatively, X is any AbstractMatrix with Continuous elements; check the scitype with scitype(X).\ny is a vector of targets whose items are of scitype Continuous. Check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.\n\nHyper-parameters\n\nSee https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html.\n\nCurrently, the following parameters and their defaults are supported:\n\nboosting::String = \"gbdt\", \nnum_iterations::Int = 100::(_ >= 0), \nlearning_rate::Float64 = 0.1::(_ > 0.), \nnum_leaves::Int = 31::(1 < _ <= 131072), \nmax_depth::Int = -1, \ntree_learner::String = \"serial\",\nhistogram_pool_size::Float64 = -1.0, \nmin_data_in_leaf::Int = 20::(_ >= 0), \nmin_sum_hessian_in_leaf::Float64 = 1e-3::(_ >= 0.0),\nmax_delta_step::Float64 = 0.0, \nlambda_l1::Float64 = 0.0::(_ >= 0.0), \nlambda_l2::Float64 = 0.0::(_ >= 0.0),\nmin_gain_to_split::Float64 = 0.0::(_ >= 0.0), \nfeature_fraction::Float64 = 1.0::(0.0 < _ <= 1.0),\nfeature_fraction_bynode::Float64 = 1.0::(0.0 < _ <= 1.0), \nfeature_fraction_seed::Int = 2, \nbagging_fraction::Float64 = 1.0::(0.0 < _ <= 1.0),\nbagging_freq::Int = 0::(_ >= 0), \nbagging_seed::Int = 3, \nearly_stopping_round::Int = 0, \nextra_trees::Bool = false,\nextra_seed::Int = 6, \nmax_bin::Int = 255::(_ > 1), \nbin_construct_sample_cnt = 200000::(_ > 0), \ndrop_rate::Float64 = 0.1::(0.0 <= _ <= 1.0),\nmax_drop::Int = 50,\nskip_drop:: Float64 = 0.5::(0.0 <= _ <= 1), \nxgboost_dart_mode::Bool = false, \nuniform_drop::Bool = false, \ndrop_seed::Int = 4,\ntop_rate::Float64 = 0.2::(0.0 <= _ <= 1.0), \nother_rate::Float64 = 0.1::(0.0 <= _ <= 1.0), \nmin_data_per_group::Int = 100::(_ > 0),\nmax_cat_threshold::Int = 32::(_ > 0), \ncat_l2::Float64 = 10.0::(_ >= 0), \ncat_smooth::Float64 = 10.0::(_ >= 0), \nobjective::String = \"multiclass\",\ncategorical_feature::Vector{Int} = Vector{Int}(), \ndata_random_seed::Int = 1, \nis_sparse::Bool = true, \nis_unbalance::Bool = false,\nboost_from_average::Bool = true,\nuse_missing::Bool = true, \nlinear_tree::Bool = false, \nfeature_pre_filter::Bool = true,\nmetric::Vector{String} = [\"none\"], \nmetric_freq::Int = 1::(_ > 0), \nis_provide_training_metric::Bool = false,\neval_at::Vector{Int} = Vector{Int}([1, 2, 3, 4, 5])::(all(_ .> 0)), \nnum_machines::Int = 1::(_ > 0), \nnum_threads::Int  = 0::(_ >= 0),\nlocal_listen_port::Int = 12400::(_ > 0), \ntime_out::Int = 120::(_ > 0), \nmachine_list_file::String = \"\", \nsave_binary::Bool = false,\ndevice_type::String = \"cpu\", \ngpu_use_dp::Bool = false, \ngpu_platform_id::Int = -1, \ngpu_device_id::Int = -1, \nnum_gpu::Int = 1,\nforce_col_wise::Bool = false, \nforce_row_wise::Bool = false, \ntruncate_booster::Bool = true.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfitresult: Fitted model information, contains a LGBMClassification object, a CategoricalArray of the input class names, and the classifier with all its parameters\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_metrics: A dictionary containing all training metrics.\nimportance: A namedtuple containing:\ngain: The total gain of each split used by the model\nsplit: The number of times each feature is used by the model.\n\nExamples\n\n\nusing DataFrames\nusing MLJ\n\n# load the model\nLGBMClassifier = @load LGBMClassifier pkg=LightGBM \n\nX, y = @load_iris \nX = DataFrame(X)\ntrain, test = partition(collect(eachindex(y)), 0.70, shuffle=true)\n\nfirst(X, 3)\nlgb = LGBMClassifier() # initialise a model with default params\nmach = machine(lgb, X[train, :], y[train]) |> fit!\n\npredict(mach, X[test, :])\n\n# access feature importances\nmodel_report = report(mach)\ngain_importance = model_report.importance.gain\nsplit_importance = model_report.importance.split\n\nSee also LightGBM.jl and the unwrapped model type LightGBM.LGBMClassification\n\n\n\n\n\n","category":"type"},{"location":"functions/#LightGBM.MLJInterface.LGBMRegressor","page":"Functions","title":"LightGBM.MLJInterface.LGBMRegressor","text":"LGBMRegressor\n\nA model type for constructing a LightGBM regressor, based on LightGBM.jl, and implementing the MLJ model interface.\n\nFrom MLJ, the type can be imported using\n\nLGBMRegressor = @load LGBMRegressor pkg=LightGBM\n\nDo model = LGBMRegressor() to construct an instance with default hyper-parameters. Provide keyword arguments to override hyper-parameter defaults, as in LGBMRegressor(boosting=...).\n\nLightGBM, short for light gradient-boosting machine, is a framework for gradient boosting based on decision tree algorithms and used for classification, regression and other machine learning tasks, with a focus on performance and scalability. This model in particular is used for various types of regression tasks.\n\nTraining data\n\nIn MLJ or MLJBase, bind an instance model to data with \n\nmach = machine(model, X, y) \n\nHere:\n\nX is any table of input features (eg, a DataFrame) whose columns are of scitype Continuous; check the column scitypes with schema(X); alternatively, X is any AbstractMatrix with Continuous elements; check the scitype with scitype(X).\ny is a vector of targets whose items are of scitype Continuous. Check the scitype with scitype(y).\n\nTrain the machine using fit!(mach, rows=...).\n\nOperations\n\npredict(mach, Xnew): return predictions of the target given new features Xnew, which should have the same scitype as X above.\n\nHyper-parameters\n\nSee https://lightgbm.readthedocs.io/en/v3.3.5/Parameters.html.\n\nCurrently, the following parameters and their defaults are supported:\n\nboosting::String = \"gbdt\", \nnum_iterations::Int = 100::(_ >= 0), \nlearning_rate::Float64 = 0.1::(_ > 0.),\nnum_leaves::Int = 31::(1 < _ <= 131072), \nmax_depth::Int = -1, \ntree_learner::String = \"serial\",\nhistogram_pool_size::Float64 = -1.0, \nmin_data_in_leaf::Int = 20::(_ >= 0), \nmin_sum_hessian_in_leaf::Float64 = 1e-3::(_ >= 0.0),\nmax_delta_step::Float64 = 0.0, \nlambda_l1::Float64 = 0.0::(_ >= 0.0), \nlambda_l2::Float64 = 0.0::(_ >= 0.0),\nmin_gain_to_split::Float64 = 0.0::(_ >= 0.0), \nfeature_fraction::Float64 = 1.0::(0.0 < _ <= 1.0),\nfeature_fraction_bynode::Float64 = 1.0::(0.0 < _ <= 1.0), \nfeature_fraction_seed::Int = 2, \nbagging_fraction::Float64 = 1.0::(0.0 < _ <= 1.0),\npos_bagging_fraction::Float64 = 1.0::(0.0 < _ <= 1.0), \nneg_bagging_fraction::Float64 = 1.0::(0.0 < _ <= 1.0),\nbagging_freq::Int = 0::(_ >= 0), \nbagging_seed::Int = 3, \nearly_stopping_round::Int = 0, \nextra_trees::Bool = false,\nextra_seed::Int = 6, \nmax_bin::Int = 255::(_ > 1), \nbin_construct_sample_cnt = 200000::(_ > 0), \ndrop_rate::Float64 = 0.1::(0.0 <= _ <= 1.0),\nmax_drop::Int = 50, \nskip_drop:: Float64 = 0.5::(0.0 <= _ <= 1), \nxgboost_dart_mode::Bool = false, \nuniform_drop::Bool = false, \ndrop_seed::Int = 4,\ntop_rate::Float64 = 0.2::(0.0 <= _ <= 1.0), \nother_rate::Float64 = 0.1::(0.0 <= _ <= 1.0), \nmin_data_per_group::Int = 100::(_ > 0),\nmax_cat_threshold::Int = 32::(_ > 0), \ncat_l2::Float64 = 10.0::(_ >= 0), \ncat_smooth::Float64 = 10.0::(_ >= 0), \nobjective::String = \"regression\",\ncategorical_feature::Vector{Int} = Vector{Int}(),\ndata_random_seed::Int = 1, \nis_sparse::Bool = true, \nis_unbalance::Bool = false,\nboost_from_average::Bool = true, \nscale_pos_weight::Float64 = 1.0, \nuse_missing::Bool = true, \nlinear_tree::Bool = false,\nfeature_pre_filter::Bool = true, \nalpha::Float64 = 0.9::(_ > 0.0 ), \nmetric::Vector{String} = [\"l2\"], \nmetric_freq::Int = 1::(_ > 0),\nis_provide_training_metric::Bool = false, \neval_at::Vector{Int} = Vector{Int}([1, 2, 3, 4, 5])::(all(_ .> 0)), \nnum_machines::Int = 1::(_ > 0),\nnum_threads::Int  = 0::(_ >= 0), \nlocal_listen_port::Int = 12400::(_ > 0), \ntime_out::Int = 120::(_ > 0), \nmachine_list_file::String = \"\",\nsave_binary::Bool = false, \ndevice_type::String = \"cpu\", \ngpu_use_dp::Bool = false, \ngpu_platform_id::Int = -1, \ngpu_device_id::Int = -1,\nnum_gpu::Int = 1, \nforce_col_wise::Bool = false, \nforce_row_wise::Bool = false, \ntruncate_booster::Bool = true.\n\nFitted parameters\n\nThe fields of fitted_params(mach) are:\n\nfitresult: Fitted model information, contains a LGBMRegression object, an empty vector, and the regressor with all its parameters\n\nReport\n\nThe fields of report(mach) are:\n\ntraining_metrics: A dictionary containing all training metrics.\nimportance: A namedtuple containing:\ngain: The total gain of each split used by the model\nsplit: The number of times each feature is used by the model.\n\nExamples\n\n\nusing DataFrames\nusing MLJ\n\n# load the model\nLGBMRegressor = @load LGBMRegressor pkg=LightGBM \n\nX, y = @load_boston # a table and a vector \nX = DataFrame(X)\ntrain, test = partition(collect(eachindex(y)), 0.70, shuffle=true)\n\nfirst(X, 3)\nlgb = LGBMRegressor() # initialise a model with default params\nmach = machine(lgb, X[train, :], y[train]) |> fit!\n\npredict(mach, X[test, :])\n\n# access feature importances\nmodel_report = report(mach)\ngain_importance = model_report.importance.gain\nsplit_importance = model_report.importance.split\n\nSee also LightGBM.jl and the unwrapped model type LightGBM.LGBMRegression\n\n\n\n\n\n","category":"type"},{"location":"#LightGBM.jl","page":"Home","title":"LightGBM.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LightGBM.jl provides a high-performance Julia interface for Microsoft's LightGBM.","category":"page"},{"location":"","page":"Home","title":"Home","text":"The package adds a couple of convenience features:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Automated cross-validation\nExhaustive grid search search procedure\nIntegration with MLJ (which also provides the above via different interfaces)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additionally, the package automatically converts all LightGBM parameters that refer to indices (e.g. categorical_feature) from Julia's one-based indices to C's zero-based indices.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A majority of the C-interfaces are implemented. A few are known to be missing and are tracked.","category":"page"},{"location":"","page":"Home","title":"Home","text":"All major operating systems (Windows, Linux, and Mac OS X) are supported. Julia versions 1.0+ are supported.","category":"page"},{"location":"#Table-of-Contents","page":"Home","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"LightGBM.jl\nTable of Contents\nInstallation\nA simple example using LightGBM example files\nParameters\nMLJ Support","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please ensure your system meets the pre-requisites for LightGBM. This generally means ensuring that libomp is installed and linkable on your system. See here for Microsoft's installation guide.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please note that the package actually downloads a precompiled binary so you do not need to install LightGBM first. This is done as a user convenience, and support will be added for supplying ones own LightGBM binary (for GPU acceleration, etc).","category":"page"},{"location":"","page":"Home","title":"Home","text":"To add the package to Julia:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.add(\"LightGBM\")","category":"page"},{"location":"","page":"Home","title":"Home","text":"Running tests for the package requires the use of the LightGBM example files, download and extract the LightGBM source and set the environment variable LIGHTGBM_EXAMPLES_PATH to the root of the source installation. Then you can run the tests by simply doing","category":"page"},{"location":"","page":"Home","title":"Home","text":"Pkg.test(\"LightGBM\")","category":"page"},{"location":"#A-simple-example-using-LightGBM-example-files","page":"Home","title":"A simple example using LightGBM example files","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"First, download LightGBM source and untar it somewhere.","category":"page"},{"location":"","page":"Home","title":"Home","text":"cd ~\nwget https://github.com/microsoft/LightGBM/archive/v3.3.5.tar.gz\ntar -xf v3.3.5.tar.gz","category":"page"},{"location":"","page":"Home","title":"Home","text":"using LightGBM\nusing DelimitedFiles\n\nLIGHTGBM_SOURCE = abspath(\"~/LightGBM-3.3.5\")\n\n# Load LightGBM's binary classification example.\nbinary_test = readdlm(joinpath(LIGHTGBM_SOURCE, \"examples\", \"binary_classification\", \"binary.test\"), '\\t')\nbinary_train = readdlm(joinpath(LIGHTGBM_SOURCE, \"examples\", \"binary_classification\", \"binary.train\"), '\\t')\nX_train = binary_train[:, 2:end]\ny_train = binary_train[:, 1]\nX_test = binary_test[:, 2:end]\ny_test = binary_test[:, 1]\n\n# Create an estimator with the desired parameters—leave other parameters at the default values.\nestimator = LGBMClassification(\n    objective = \"binary\",\n    num_iterations = 100,\n    learning_rate = .1,\n    early_stopping_round = 5,\n    feature_fraction = .8,\n    bagging_fraction = .9,\n    bagging_freq = 1,\n    num_leaves = 1000,\n    num_class = 1,\n    metric = [\"auc\", \"binary_logloss\"]\n)\n\n# Fit the estimator on the training data and return its scores for the test data.\nfit!(estimator, X_train, y_train, (X_test, y_test))\n\n# Predict arbitrary data with the estimator.\npredict(estimator, X_train)\n\n# Cross-validate using a two-fold cross-validation iterable providing training indices.\nsplits = (collect(1:3500), collect(3501:7000))\ncv(estimator, X_train, y_train, splits)\n\n# Exhaustive search on an iterable containing all combinations of learning_rate ∈ {.1, .2} and\n# bagging_fraction ∈ {.8, .9}\nparams = [Dict(:learning_rate => learning_rate,\n               :bagging_fraction => bagging_fraction) for\n          learning_rate in (.1, .2),\n          bagging_fraction in (.8, .9)]\nsearch_cv(estimator, X_train, y_train, splits, params)\n\n# Save and load the fitted model.\nfilename = pwd() * \"/finished.model\"\nsavemodel(estimator, filename)\nloadmodel!(estimator, filename)","category":"page"},{"location":"#Parameters","page":"Home","title":"Parameters","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Note that a lot of parameters used within this module and in the code and examples are exact matches with those from LightGBM. Not all of these are necessarily supported but see the guide for detailed explanations of what these parameters do and their valid values.","category":"page"},{"location":"#MLJ-Support","page":"Home","title":"MLJ Support","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package has an interface to MLJ. Exhaustive MLJ documentation is out of scope for here, however the main things are:","category":"page"},{"location":"","page":"Home","title":"Home","text":"The MLJ interface models are","category":"page"},{"location":"","page":"Home","title":"Home","text":"LightGBM.MLJInterface.LGBMClassifier\nLightGBM.MLJInterface.LGBMRegressor","category":"page"},{"location":"","page":"Home","title":"Home","text":"And these have the same interface parameters as the estimators","category":"page"},{"location":"","page":"Home","title":"Home","text":"The interface models are generally passed to MLJBase.fit or MLJBase.machine and integrated as part of a larger MLJ pipeline. An example is provided","category":"page"},{"location":"#Custom-LightGBM-binaries","page":"Home","title":"Custom LightGBM binaries","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Though this package comes with a precompiled binary (lib_lightgbm.so for linux, lib_lightgbm.dylib for macos, lib_lightgbm.dll for windows, refer to Microsoft's LightGBM release page), a custom binary can be used with this package (we use Libdl.dlopen to do this). In order to do so, either:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Add the directory of your custom binary to the Libdl.DL_LOAD_PATH before calling import LightGBM, e.g.   ```   import Libdl   push!(Libdl.DLLOADPATH, \"/path/to/your/lib_lightgbm/directory\")\nimport LightGBM   ...   ```\nSpecify the directory of your custom binary in the environment variables LD_LIBRARY_PATH (for linux), DYLD_LIBRARY_PATH (macos), PATH (windows), or place the custom binary file in the system search path","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note: Libdl.DL_LOAD_PATH will be first searched and used, then the system library paths. If no binaries are found, the program will fallback to using the precompiled binary","category":"page"},{"location":"#Contributors","page":"Home","title":"Contributors ✨","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Please don't hesitate to add yourself when you contribute to CONTRIBUTORS.md.","category":"page"}]
}
